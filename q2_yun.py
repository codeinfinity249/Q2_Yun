# -*- coding: utf-8 -*-
"""Q2_Yun.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/129QMTpRfFvJPReS1AOM3GKLuwOLe7AZN
"""

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam

# Parameters
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 0.0001

# Dataset directories
train_dir = 'path_to_unzipped_dataset/train'  # Replace with the path to the 'train' folder
test_dir = 'path_to_unzipped_dataset/test'    # Replace with the path to the 'test' folder

# Data augmentation and preprocessing
train_datagen = ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = ImageDataGenerator(rescale=1.0 / 255)

train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

test_data = test_datagen.flow_from_directory(
    test_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

# Load pre-trained MobileNetV2 and freeze base layers
base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')
base_model.trainable = False

# Add custom layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(train_data.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

# Compile the model
model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model (fine-tuning only the added layers)
model.fit(
    train_data,
    validation_data=test_data,
    epochs=EPOCHS
)

# Unfreeze the base model and fine-tune
base_model.trainable = True
model.compile(optimizer=Adam(learning_rate=LEARNING_RATE / 10),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the entire model
model.fit(
    train_data,
    validation_data=test_data,
    epochs=EPOCHS // 2
)

# Save the model
model.save('fer_emotion_recognition_model.h5')

print("Model training and fine-tuning completed!")